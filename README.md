# Pintrest Data Pipeline

## Contents
1) Introduction
2) Instructions
3) 

## Introduction
Pinterest crunches billions of data points every day to decide how to provide more value to their users. In this project, the AWS cloud is utilised to mimic this system.

This project demonstrates the following skills:
Milestone 1 & 2:
1) Connecting and navigating to AWS
BATCH PROCESSING
Milestone 3
1) Configuring an Amazon EC2 instance to use as an Apache Kafka client machine
2) Configuring EC2 Kafka Client
Milestone 4
1) Use MSK connect to connect MSK cluster to S3 bucket.
Milestone 5
4) Configuring an API in API Gateway. API will send data to the MSK Cluster, through the connector, storing data in the S3 bucket.
Milestone 6 
1) Read data from AWS into Databricks

## Instructions

### Setup
1) To utilise this project clone the repository from GitHub onto local machine
```
git clone https://github.com/Deepa-RA/pinterest-data-pipeline804.git
```
2) Ensure the project is run within the repository
```
cd pinterest-data-pipeline804/
```
### Running the project
1) The directory /documentation contains guidance for completing each milestone
2) The directory /Databricks_notebooks contains notebooks used in Databricks for Batch processing the data